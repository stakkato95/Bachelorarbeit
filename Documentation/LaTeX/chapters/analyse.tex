\chapter{Analyse}
\label{cha:Analyse}

Um die Problemstellung der Arbeit besser zu verstehen, muss man einige grundlegende Konzepte der \textit{Schwarmrobotik} und verteilter Systeme verstehen. Außerdem gibt dieses Kapitel einen Überblick auf den aktuellen Stand der Technik, Optionen und Empfehlungen für technische Umsetzung.

\section{Vorteile von Drohnenschwärmen}
\label{cha:prosCons}

Heutzutage werden Drohnen entweder einzeln verwendet oder sie finden in einem Drohnenschwarm ihre Anwendung. Beide Verwendungsarten haben Vor- und Nachteile und umfassende Anwendungsfälle. Um den Weg von einer Drohne zu einem Drohnenschwarm zu verstehen, muss man zuerst die Forschungsrichtungen im Feld von UAVs \footnote{Ein UAV ist ein Luftfahrzeug, das ohne eine an Bord befindliche Besatzung autark durch einen Computer oder vom Boden über eine Fernsteuerung betrieben und navigiert werden kann.} (Unmanned Aerial Vehicles) betrachten. Zu UAVs gibt es vier Forschungsschwerpunkte, die an entsprechende Herausforderungen gebunden sind:

\begin{enumerate}
	\item Ein einziges UAV muss stets in der Lage sein, bestimmte \textit{Gebiete und Flächen zu überfliegen}.
	\item \textit{Objekte und Ereignisse von Interesse} müssen von dem UAV erkannt werden können.
	\item Um einen Synergieeffekt zu erreichen, ist die \textit{Kommunikation} zwischen mehreren Drohnen essenziell.
	\item Damit das Arbeiten der Drohnen an einem gemeinsamen Ziel ermöglicht wird, gilt es die \textit{Koordination} eines Netzwerks von UAVs zu erreichen.
\end{enumerate}

Diese Bachelorarbeit fokussiert sich auf Drohnenschwärme und auf die Koordination zwischen den einzelnen Drohnen. Betrachtet man eine Gruppe von Drohnen, stellt man fest, dass man einen Algorithmus benötigt, der die Funktionsweise dieser Gruppe optimiert, damit diese durch Zusammenarbeit ein gemeinsames Ziel erreicht. Unter anderem können folgende Probleme durch die Anwendung eine Schwarms gelöst werden:

\begin{description} 
	\item[Verteilung von Rechenoperationen (optimale Nutzung von Ressourcen).] Das ist ein klassischer Anwendungsfall aus der Informatik. Keine einzelne Drohne hat genug Rechenleistung, um eine schwierige Operation im Stand-Alone-Betrieb bzw. ohne Verbindung zur Cloud zu berechnen. Dadurch, dass es in einem Schwarm mehrere Drohnen gibt, kann die Arbeit aufgeteilt werden. So einen Ansatz kann man \textit{"Ad-Hoc-Cloud"} nennen. Unter der optimalen Nutzung von Ressourcen kann man auch den Austausch von lokalen Karten und Sensorwerten verstehen.
	
	\item[Verteilung von Aufgaben.] Für eine bestimmte Art von Problemen, die Schwarmintelligenz zur Lösung benötigt, ist es wünschenswert, dass jede Drohne eine bestimmte Funktion übernimmt und durch ihre speziellen Fähigkeiten (künstliche Intelligenz) oder Ausrüstung (Sensoren und Aktuatoren) hilft, das gemeinsame Ziel zu erreichen.
	Aus der Sicht der Ausfallsicherheit, Robustheit und Zuverlässigkeit eines verteilten Systems ist es immer besser, Tasks auf mehrere Geräte zu verteilen.  Dieser Zusammenhang wurde schon durch die Erfahrungen von Google mit verteilten Systemen bewiesen \cite{chubby}. Andererseits muss man mit Kommunikationsoverhead und gestiegener Komplexität bei der Entwicklung rechnen.
	
	\item[Ausfallsicherheit.] Ein Drohnenschwarm ist robust gegen Teilausfälle und bietet mehr Flexibilität bei Missionen. Wie oben schon erwähnt, können Aufgabenbereiche auf einzelne Drohnen und Drohnengruppen aufgeteilt werden.
	
	\item[Kollektive Entscheidungsfindung.]	Jede Drohne im Schwarm kann bzw. darf nur einen Teil vom ganzen Problem kennen. Für die Entscheidungsfindung braucht ein automatisiertes System sehr häufig nicht nur das lokale Wissen, sondern auch die Vogelperspektive oder den “Meinungsaustausch” mit anderen Drohnen.
\end{description}

\section{Autonomie im Drohnenschwarm}
\label{cha:swarmAutonomy}

Vorteile eines Schwarms entstehen hauptsächlich durch die Kommunikation zwischen Drohnen. Ein wichtiger Aspekt dabei ist die Autonomie von Drohnen. Autonomie ermöglicht einzelne Drohnen eines Schwarms lokale Entscheidungen zu treffen. 

\subsection{Methoden zur Kommunikation im Drohnenschwarm}

Um die Vorteile, die Autonomie einzelner Drohnen mit sich bringt, ausnutzen zu können, können folgende systematische Ansätze angewendet werden:

\begin{description} 
	\item[Multiagentensystem.] Ein Multiagentensystem ist eine Klasse von Algorithmen, in der einzelne Agenten basierend auf vordefinierten Regeln und Einschränkungen miteinander interagieren. Dadurch wird ein kollektives Verhalten ermöglicht \cite{Salamon2011}.
	
	Die Interaktionen im System finden sowohl zwischen Agenten untereinander als auch zwischen Agenten und der Umgebung statt. Dabei spielt die sogenannte \textit{"Reward Function"} eine wichtige Rolle \cite{Panait05cooperativemulti-agent}. Diese beschreibt, wie sich ein Agent verhalten muss. Der \textit{"normative Inhalt"} schreibt dem Agenten vor, wie er bestimmte Aufgaben lösen soll. Im Multiagentensystem kennt ein Agent den gesamten Problembereich bzw. -raum  nicht und muss die Lösung deswegen durch Lernen herausfinden. Multiagentensysteme üben Selbstorganisation, komplexe Verhaltensweisen und auch Kontrollparadigmen aus, obwohl individuelle Strategien von allen Agenten einfach sind.
	
	Ein Agent im Multiagentensystem hat einige wichtige Eigenschaften: \textit{Autonomie} (ein Agent ist teilweise unabhängig, selbstbewusst und autonom), \textit{lokaler Überblick} (kein Agent hat einen globalen Überblick und das System ist für einen einzelnen Agent zu komplex) und \textit{Dezentralisierung} (kein Agent wird als Leader bezeichnet / gewählt).
	
	\item[Verteilte Problemlösung.] Kooperative verteilte Problemlösung ist ein Netzwerk von halbautonomen Verarbeitungsknoten, die zusammenarbeiten, um ein Problem zu lösen \cite{Shoham:2009p113}. Dabei geht es um die Untersuchung der Problemaufteilung, Unterproblemaufteilung, Synthese vom Ereignis, Optimierung des Problemlösers und Koordination. Es ist eng mit der verteilten Programmierung und der verteilten Optimierung verbunden.
	
	In der verteilten Problemlösung arbeiten mehrere Agenten daran, ein spezifisches Problem zu lösen. Das wichtigste in diesen Systemen ist, dass Kooperation erforderlich ist, weil kein einziger Agent genug Informationen, Wissen und Fähigkeiten hat, um das Problem zu lösen \cite{Shoham:2009p113}. Die eigentliche Herausforderung besteht in der Sicherstellung, dass die Informationen so aufgeteilt werden, dass die Agenten einander ergänzen und nicht miteinander im Konflikt stehen \cite{Shoham:2009p113}. Ein Algorithmus für die verteilte Problemlösung muss ein größeres Problem in Teilaufgaben unter Berücksichtigung der räumlichen, zeitlichen oder funktionalen Aspekte gliedern \cite{Shoham:2009p113}.
	
	Ein Algorithmus für verteilte Problemlösung muss unter folgenden Einschränkungen handeln:
	\begin{enumerate}
		\item Kein Knoten hat genug Informationen, um das Problem selbstständig zu lösen.
		
		\item Im System gibt es weder eine globale Steuerung, noch einen Speicher. Steuerung und Speicher sind verteilt.
		
		\item Die Berechnung von Operationen auf einer lokalen CPU ist grundsätzlich schneller und weniger aufwändig als die Aufteilung derselben Operationen auf verteilte Systeme.
		
		\item Darüber hinaus muss das zu lösende Problem modular sein. Außerdem darf es keinen einzigartigen Knoten geben, da dies zu einem \textit{Bottleneck} im System führen kann. Ein solcher \textit{einzigartiger Knoten} wäre jener, der im Cluster oder im Schwarm eine spezielle Rolle erfüllt. Eine solche Aufgabe könnte beispielsweise die eines Leaders oder eines Koordinators sein.
	\end{enumerate}
	
	
	\item[Schwarmintelligenz / Schwarmrobotik.] Schwarmintelligenz ist das kollektive Verhalten von dezentralisierten, selbstorganisierten, natürlichen oder künstlichen Systemen. Das Konzept wird sehr oft im Bereich der Künstlichen Intelligenz eingesetzt.
	
	Systeme mit Schwarmintelligenz bestehen typischerweise aus einer Population von Agenten \cite{Garnier07thebiological}, die untereinander und mit der Umgebung kommunizieren. Inspiration für solche Systeme kommt aus der Natur, vor allem aus biologischen Systemen. Die Agenten folgen sehr einfache Regeln. Obwohl es keine zentralisierte Steuerung gibt, führen lokale und bis zu einem gewissen Grad zufällige Interaktionen zur Entstehung eines intelligenten und globalen Verhaltens. Ein klassisches Beispiel aus der Natur sind Ameisenkolonien.
	
	An dieser Stelle sieht man, dass die Grenzen zwischen Multiagentensystem, verteilter Problemlösung und Schwarmintelligenz verschwimmen können. In der Tat ist das Bilden einer klaren Grenze sehr schwierig.
\end{description}

\subsection{Stand der Technik}

Moderne Systeme sind meist mit der Cloud verbunden oder brauchen zumindest eine kurze Synchronisation mit einem Cloud-Server. Nehmen wir an, dass eine Drohne eine Mission erfüllen soll, was sowohl Kommunikation zwischen Drohnen als auch zwischen dem Schwarm und der Cloud erfordert. Alle eingebetteten Systeme sind bis zu einem gewissen Grad in ihren Ressourcen (Rechenleistung, Akku, Konnektivität, Speicher) eingeschränkt. Dennoch wäre es sinnvoll, \textit{Missionsinformationen} zwischen der Cloud und einer einzelnen Drohne (dem sogenannten Leader) zu synchronisieren und mittels eines Algorithmus im Schwarm zu verteilen.

Dies widerspricht dem Regel, dass jeder einzigartige Knoten in einem verteilten System ein potenzielles Bottleneck ist, da die Präsenz eines speziellen Knotens die Durchsatzrate senken und die Ausfallsicherheit des Gesamtsystems verringern kann. Diese Herausforderungen müssen natürlich mit einem klugen Algorithmus ausgeglichen werden, womit sich diese Arbeit auch befasst.

In der Zukunft sollen Systeme wie Drohnenschwärme selbstorganisierend und autonom agieren. Laut den Forschungsergebnissen der Technischen Hochschule Lausanne (EPFL) sind allerdings mit aktuellem Stand der Technik in der Robotik 100\% selbstorganisierende Systeme weniger effizient als die Systeme, in denen Kommunikation zwischen Knoten möglich ist \cite{EPFLswarm}. Das heißt, dass die Synchronisation von Knoten im Schwarm eine gültige und in der Branche durchaus akzeptierte Lösung ist.

\section{Analyse von Konsens Algorithmen für verteilte, ausfallsichere Systeme}
\label{cha:concurrency}

Wie bereits im vorherigen Kapitel festgestellt, ist für das Funktionieren eines Schwarms der \textit{Konsens} zwischen den Knoten erforderlich. Die Auswahl eines Konsens-Algorithmus beeinflusst die Robustheit des Gesamtsystems und wird in diesem Kapitel betrachtet.

Verteilte Systeme finden in mehreren Teilen der IT-Infrastruktur Einsatz. Häufige Einsatzbereiche sind:

\begin{enumerate}
	\item Dateisysteme (Ceph, IPFS, HDFS)
	
	\item Datenbanken (relationale Datenbanken, NoSQL und NewSQL Datenbanken)
	
	\item Key-Value Stores (Schlüssel-Werte-Datenbank oder eine verteilte Hashtabelle wie DynamoDB, etcd und Redis)
	
	\item Queues (Verarbeitung von Datenströmen, keine einfache Datenstruktur in diesem Kontext; RabbitMQ, Kafka)
\end{enumerate}

Diese genannten Systeme haben zwar unterschiedliche Anwendungsgebiete, die potenziell auftretenden Herausforderungen sind allerdings sehr ähnlich \cite{Buyya08cloudcomputing}:

\begin{description} 
	\item[Fehlertoleranz.] Um die Fehlertoleranz zu garantieren, muss ein System mehrere \textit{Replica} aller Daten besitzen und beim Ausfall der \textit{Hauptreplica} auf eine andere reibungslos und ohne Datenverlust umsteigen können. Die Anzahl von Replicas soll immer eine ungerade Zahl sein, typischerweise werden drei oder fünf Replicas gebildet. Eine ungerade Anzahl verhindert bei Systemen, in denen der korrekte Wert durch eine Abstimmung zwischen allen Replicas bestimmt wird, eine gleiche Aufteilung von Stimmen zwischen 2 Gruppen von Replicas. Gibt es eine Mehrheit aller Knoten, die den gleichen Wert haben, werden diese Daten als korrekt gewertet.
	
	\item[Zeit für Wiederherstellung.] Ein Algorithmus für verteilte Systeme muss die Zeit für die Wiederherstellung nach einem Ausfall verkürzen. Der Absturz bringt mit sich Einschränkungen, nach denen das System sich anpassen muss.
	
	\item[Erreichbarkeit.] Die Erreichbarkeit eines Systems stellt fest, ob ein Knoten als abgestürzt oder als laufend betrachtet werden soll \cite{Armbrust10aview}. In verteilten Systemen gibt es allerdings keine Möglichkeit zu überprüfen, ob ein Knoten abgestürzt ist oder das Netzwerk dieses Knotens überfordert ist. In modernen Systemen wird das Problem mittels \textit{Timer} gelöst. Nach einem bestimmten Timeout wird der Knoten als nicht mehr erreichbar markiert. Nach welcher Zeit ein Knoten als nicht erreichbar betrachtet wird, ist für jede Konfiguration des Clusters spezifisch.
	
	\item[Skalierbarkeit und Durchsatzrate.] Die Skalierbarkeit und Durchsatzrate in verteilten Systemen wird durch “Scaling out” Verfahren erreicht. "Scaping out" bedeutet, dass zur Lösung eines Problems mehrere Knoten zusammengebracht werden. Für ein klassisches “Scaling out” Verfahren braucht man dagegen einen neuen, leistungsfähigeren Rechner, der alle Anfragen abarbeiten kann. Eine weitere Lösungsmöglichkeit stellen verteilte Algorithmen dar. Ein Algorithmus, der Leseoperationen von allen Knoten eines Clusters erlaubt, ist viel effizienter als Algorithmus, der Leseoperationen nur von einem einzigen Knoten im Cluster erlaubt. Dieser einzige Knoten ist leicht überfordert und wird durch eine hohe Nutzung des Netzwerks öfter als andere Knoten ausfallen.
	
	\item[Synchronisation und Timestamping.] Timestamping hilft bei der Auflösung von Konflikten, beispielsweise zwischen Einträgen auf unterschiedlichen Knoten einer verteilten Datenbank bzw. eines verteilten Dateisystems. Der Eintrag mit dem höheren Timestamp hat Vorrang und soll einen anderen Eintrag überschreiben. Das Problem mit den Timestamps ist die Desynchronisation von Uhren auf allen Knoten. Mit modernen Technologien ist eine vollständige Synchronisation zwischen allen Knoten nicht umsetzbar. Selbst für Google, das mehrere Datenzentren mit Atomuhren und GPS-Fehlerkorrektur betreibt, ist das derzeit unmöglich. Die \textit{TrueTime API} liefert zwar für höchst genaue Timestamps einen Zeitabstand, diese sind allerdings nicht eindeutig.
	
	Eine weitere Lösung bietet die sogenannte \textit{"Logische Anordnung von Operationen"}, welche Algorithmen zur Verfügung stellt. Es werden logische Zusammenhänge zwischen Operationen festgestellt und dadurch eine Linearisierung jener Operationen erreicht. (Anordnung dieser Operationen, als ob sie nacheinander ausgeführt wären).
\end{description}

Bis auf die Erreichbarkeit können all diese Probleme durch die Kombination spezialisierter Algorithmen gelöst werden. Algorithmen zur Replikiation von Daten zwischen mehreren Knoten eines Schwarms fallen in die Klasse der Konsens-Algorithmen. Diese Klasse von Algorithmen löst ein fundamentales Problem der verteilten Systeme und Multiagentensysteme: die Sicherstellung der allgemeinen Zuverlässigkeit des Systems beim Vorhandensein einer größeren Anzahl fehlerhafter Knoten. Dies erfordert eine Koordinierung der Knoten, um einen Konsens zu erreichen bzw. sich über einen Wert zu einigen, der für eine Rechenoperation nötig ist. Mögliche Anwendungsbereiche von Konsens-Algorithmen umfassen:

\begin{enumerate}
	\item Anordnung von Transaktionen einer Datenbank
	
	\item Cloud Computing
	
	\item Lastverteilung (wie Load Balancing für Internetdienste)
	
	\item Blockchain
	
	\item Steuerung von UAVs (und generell mehreren Robotern / Agenten)
\end{enumerate}

Um die richtige Entscheidung bei der Auswahl eines Algorithmus für den Konsens zu treffen, muss man zuerst die dahinter liegenden theoretischen Grundlagen verstehen, die auch Auswahlkriterien für Konsens-Algorithmen darstellen.

\subsection{Konsens in verteilten Systemen}

Konsens ist ein grundlegendes Problem bei der Steuerung von Multiagentensystemen. Definition von Konsens besagt, dass alle Knoten sich auf einen Mehrheitswert einigen müssen \cite{Saber03consensusproblems}. In diesem Zusammenhang ist es erforderlich, dass sich im verteilten System eine Mehrheit bildet. Die Mehrheit ist immer um einen Knoten größer als die Hälfte aller Knoten, die am Wahlprozess teilnehmen. Ein oder mehrere fehlerhafte Knoten können das resultierende Ergebnis jedoch so verzerren, dass möglicherweise kein Konsens oder ein falsches Ergebnis erzielt wird.

Algorithmen, die den Konsens lösen, wurden entwickelt, um mit einer begrenzten Anzahl fehlerhafter Knoten umzugehen. Ein Knoten wird als fehlerhaft bezeichnet, wenn in diesem Knoten ein Fehler auftritt, durch den der Knoten nicht mehr erreichbar wird. Diese Algorithmen müssen eine Reihe von Anforderungen erfüllen, um nützlich zu sein. Ein Konsens-Algorithmus, der fehlerhafte Knoten toleriert, muss folgende Kriterien erfüllen \cite{Fischer83impossibilityof}:

\begin{description} 
	\item[Termination.] Jeder korrekte Knoten entscheidet sich für einen Wert.
	
	\item[Integrity.] Wenn alle korrekten Knoten denselben Wert vorschlagen, muss sich jeder korrekte Knoten für diesen Wert entscheiden.
	
	\item[Agreement.] Jeder korrekte Knoten muss sich auf den gleichen Wert einigen.
\end{description}

All diese Kriterien werden durch die grundlegende Eigenschaften verteilter Systeme geprägt \cite{Chandra96unreliablefailure}. Von diesen Eigenschaften zeichnet man vor allem aus:


\begin{description} 
	\item[Atomarität.] Die wichtigste Eigenschaft aller verteilten Systeme besteht darin, dass ein System auf einen externen Beobachter wie ein einziger Rechner wirken soll. Das System verbirgt Implementierungsdetails und dient als eine Abstraktionsschicht.
	
	\item[Widerstandsfähigkeit.] Wie es schon erwähnt wurde, braucht ein System mehrere Replicas, um einem Ausfall zu widerstehen. Die Widerstandsfähigkeit, also die Anzahl der Fehler, die ein System überstehen kann, wird mit folgender Formel ausgerechnet:
	
	\[
	T = (n - 1) / 2
	\]
	bei der
	\begin{conditions*}
		T  &  Anzahl der überstandenen Fehler \\
		n  &  Anzahl der Knoten im Cluster bzw. Drohnenschwarm \\
	\end{conditions*}
\end{description}

Nach einem sogenannten \textit{"brain split"}, also wenn eine Verbindung zwischen zwei Gruppen von Knoten unterbrochen wird, wird nur die Gruppe die über die Mehrheit aller Knoten verfügt, weiter aktuelle Daten liefern \cite{Chandra96unreliablefailure}.

Neben den Eigenschaften müssen auch grundlegende Konzepte für Modellierung verteilter Systeme in Betracht gezogen werden. Zu den grundlegendsten und für das Konsensproblem am relevantesten gehören:

\begin{description} 
	\item[State Machine Replication (Replikation eines Zustandsautomaten).] In der Informatik ist die Replikation eines Zustandsautomaten eine allgemeine Methode zur Implementierung eines fehlertoleranten Dienstes durch Replikation von Knoten, Koordinierung der Clients und der Replicas \cite{Schneider90implementingfault-tolerant}. Der Ansatz definiert auch die Richtlinien für das Verständnis und die Entwicklung von Algorithmen für Replikation der Zustandsautomaten.
	
	Die Funktionsweise eines replizierten Zustandsautomaten erfordert folgende Schritte:
	
	\begin{enumerate}
		\item Installation der Zustandsautomaten auf mehrere unabhängige Knoten
		
		\item Empfang der Client Anfragen, die als Eingabedaten für den Zustandsautomaten dienen
		
		\item Anordnung der Eingabedaten
		
		\item Anwendung der Eingabedaten nach der ausgewählten Reihenfolge auf jedem Knoten
		
		\item Den Clients mit den Ausgabedaten des Zustandsautomaten antworten
		
		\item Beobachtung und Überprüfung der Replicas auf Zustandsabweichnungen oder Abweichungen in den Ausgabedaten
	\end{enumerate}

	Die Replikation eines Zustandsautomaten ist ein rein theoretisches Konzept, es teilt aber das große Problem der fehlertoleranten Replikation in Subprobleme auf \cite{Schneider90implementingfault-tolerant}. Bei der Lösung dieser Subprobleme können gewisse Kompromisse getroffen werden, welche sich dann als betriebliche Beschränkungen eines Produktivsystems bemerkbar machen könnten. Der 2. Schritt braucht beispielsweise einen Mechanismus, um die Nachrichten zuverlässig zu senden (\textit{Atomic Broadcast}). 3. Schritt - ein theoretisches Framework zur Linearisierung aller Operationen (\textit{eine lineare Anordnung aller Operationen}, als ob sie sequenziell und nicht parallel angewandt wurden, was für hochbelastete verteilte Systeme sehr problematisch ist), 4. Schritt - Definition \textit{eines Zustandsautomaten}, 6. Schritt - ein Algorithmus für Fehlererkennung.
	
	\item[Atomic Broadcast (auch Total Order Broadcast).] Es ist ein Broadcast, bei dem alle korrekten Knoten Nachrichten in derselben Reihenfolge empfangen \cite{Défago04totalorder}. Die Sendung wird als "atomar" bezeichnet, weil sie entweder an allen Knoten korrekt ankommt oder alle Teilnehmer das Senden abbrechen. Das beschreibt keine spezifische Implementierung, es ist lediglich ein Konzept und ein wichtiger Bestandteil des verteilten Rechnens. Die drei wichtigsten formalen Eigenschaften des Atomic Broadcast sind:
	
	\begin{enumerate}
		\item \textit{Validity} (Gültigkeit). Wenn ein korrekt funktionierender Knoten eine Nachricht abschickt, dann empfangen alle anderen Knoten die Nachricht unvermeidlich.
		
		\item \textit{Agreement} (Einigung). Wenn ein korrekt funktionierender Knoten eine Nachricht empfängt, dann empfangen die Nachricht auch alle anderen Knoten.
		
		\item \textit{Total order} (Anordnung). Die Anordnung aller empfangenen Nachrichten ist bei allen Knoten gleich.
	\end{enumerate}
	
	In der Realität kann Atomic Broadcast nicht implementiert werden, da die Netzwerke, in denen verteilte Systeme funktionieren, nicht zu 100\% zuverlässig sind, selbst wenn der Netzbetreiber sich sehr bemüht, oder Pakete ein privates Netzwerk nicht verlassen. Betrachtet man Atomic Broadcast nüchtern, muss er in zwei Subkonzepte aufgeteilt werden: \textit{Reliable Broadcast} und \textit{Konsens}. Diese beiden Themen bilden den Schwerpunkt dieses Kapitels.
	
	\item[Reliable Broadcast (zuverlässiger Broadcast).] Das Ziel dieses Broadcasts ist einfach: eine Nachricht zuverlässig über einen Cluster zu verbreiten, sodass alle Knoten die Nachricht empfangen und genau einmal verarbeiten \cite{Défago04totalorder}. \textit{"Zuverlässig"} bedeutet, dass es auch dann funktionieren sollte, wenn die Verbindungen zwischen Knoten fehlerhaft sind. Eine wichtige Annahme beim Reliable Broadcast lautet: wenn ein Prozess abstürzt und nicht mehr erreichbar ist oder die Netzverbindung einen Fehler erzeugt, wird irgendwann in der Zukunft dieser Prozess erneut gestartet bzw. das Netz wird wieder in Betrieb genommen und nach einem Versuch wird die Nachricht letztendlich zugestellt. Reliable Broadcast ist einfach zu implementiert und übernimmt nur zwei Eigenschaften des Atomic Broadcasts: \textit{Validity} und \textit{Agreement}. Total Order ist die Aufgabe des Konsenses.
	
	\item[Consensus (Konsens).] Wie am Anfang dieses Kapitels schon erwähnt, erfordert Konsens die Einigung zwischen Prozessen über die Reihenfolge der in den Cluster geschriebenen Werte. Wären alle Knoten und das Netzwerk zu 100\% korrekt und zuverlässig, bräuchte man den Konsens nicht.
\end{description}

Diese theoretischen Bausteine sind Grundlagen für zwei Algorithmen, die für die Auflösung des Konsenses in verteilten Systemen besonders wichtig sind: \textit{Paxos} und \textit{Raft}.

\subsection{Paxos und Raft für Konsens in verteilten Systemen}

Das Wichtigste bei Konsens-Algorithmen ist es zu verstehen, dass \cite{Saber03consensusproblems}:

\begin{enumerate}
	\item Konsens die Eignung auf einen einzelnen Wert bedeutet
	
	\item der Konsens erreicht ist, wenn sich die Mehrheit auf einen Wert einigt
	
	\item jeder Knoten im System den erreichten Konsens irgendwann erfahren muss
	
	\item die Kommunikationskanäle fehleranfällig sind, was die Erreichung des Konsenses verhindern kann
\end{enumerate}

Generell gibt es zwei Klassen von Systemen, die auf Konsens-Algorithmen beruhen:

\begin{description} 
	\item[Leader-Replica Schema.] In diesem System gibt es immer einen \textit{Leader}, der Anfragen beantwortet und Schreiboperationen steuert und Replicas (Followers), die Daten eines Leaders replizieren. In diesem Fall müssen alle Knoten nur dann einen Konsens erreichen, wenn der alte Leader abstürzt oder aus einem anderen Grund unerreichbar wird. In der Zwischenzeit, während noch kein neuer Leader ausgewählt ist, ist das System für neue Anfragen nicht erreichbar.
	
	\item[Peer-to-Peer Schema.] In diesem System verhalten sich alle Knoten gleich und sind gleichrangig. Wenn mehrere Clients einige Werte schreiben wollen, müssen sich die Knoten untereinander einigen, in welcher Reihenfolge neue Werte im System gespeichert werden. Das ist wichtig, da alle Knoten dieselbe Reihenfolge gespeicherter Werte einhalten müssen. In diesem Fall muss der Konsens kontinuierlich aufrechterhalten werden.
\end{description}

Paxos \cite{Lamport98thepart-time} und Raft \cite{Ongaro14insearch} gehören zur \textit{"Leader-Replica Schema"} Klasse von Konsens-Algorithmen. Zusätzlich zu den oben genannten Eigenschaften (Atomarität, Widerstandsfähigkeit), verfügen Konsens-Algorithmen auch über folgenden Eigenschaften:

\begin{enumerate}
	\item Sie sorgen für Sicherheit unter allen \textit{nicht byzantinischen Bedingungen} \footnote{Unter byzantischen Bedingungen versteht man die Situationen, wenn ein Knoten sich abhängig vom Beobachter entweder als ein korrekter oder als ein fehlerhafter Prozess verhalten kann}, einschließlich Netzwerk-Verzögerungen, Partitionen des Netzwerks, Paketverlust, Deduplizierung und Neuordnung der Pakete. Sicherheit in diesem Kontext bedeutet, dass das System immer einen korrekten Wert zurückgibt. Ein korrekter Wert in verteilten Systemen ist der Wert, der auf einem \textit{Quorum}\footnote{Quorum ist die Mehrheit aller Knoten in einem Cluster.} aller Knoten repliziert wurde. Ein Wert, der nur auf einem einzigen Knoten gespeichert wurde und dann an einen Client zurückgeschickt wird, ist nicht korrekt.
	
	\item Sie sind funktionsfähig, solange das Quorum betriebsbereit ist und die Knoten mit Clients kommunizieren können. Somit kann ein typischer Cluster von fünf Knoten den Ausfall von zwei beliebigen Knoten tolerieren. Es wird angenommen, dass Knoten durch einen Absturz unerreichbar werden. Sie können sich später von einem zwischengespeicherten Zustand wiederherstellen und wieder dem Cluster beitreten.
	
	\item Ein Konsens-Algorithmus hängt nicht vom \textit{Timing} ab, um die Konsistenz sicherzustellen: fehlerhafte Uhren und extreme Nachrichtenverzögerungen können schlimmstenfalls zu Verfügbarkeitsproblemen führen. Eine Minderheit langsamer Knoten muss sich nicht auf die Gesamtsystemleistung auswirken.
\end{enumerate}

\subsection{Der Paxos-Algorithmus}

Paxos ist eine Familie von Algorithmen zur Lösung des Konsenses in einem Netzwerk unzuverlässiger Knoten. Der Paxos-Algorithmus wurde erstmals 1989 vorgestellt und nach einem fiktiven legislativen Konsenssystem auf der griechischen Insel Paxos benannt \cite{Lamport98thepart-time}. Er wurde später als Artikel im Jahre 1998 veröffentlicht. Der ursprüngliche Algorithmus des Erfinders Leslie Lamport nennt man üblicherweise \textit{Singledecree-Paxos}.

Die Paxos-Algorithmen \cite{Lamport2001paxos-made-simple} umfassen ein Spektrum von Kompromissen zwischen der Anzahl der Knoten, der Anzahl der Round-Trip-Messages, der Aktivitäten einzelner Knoten, der Anzahl der gesendeten Nachrichten und den möglichen Fehlern. Obwohl kein deterministischer, fehlertoleranter Konsens-Algorithmus den \textit{Fortschritt} (fehlerfreie Operation des Algorithmus) in einem asynchronen Netzwerk garantieren kann, sichert Paxos \textit{Safety} (das System gibt immer einen korrekten Wert zurück).

Beim Singledecree-Paxos \cite{Renesse15paxosmade} hängen die Operationen, die Knoten ausführen können, von ihren Rollen ab:

\begin{description} 
	\item[Client.] Ein Client schickt eine Anfrage an das verteilte System und wartet auf eine Antwort.
	
	\item[Acceptor (Akzeptor).] Acceptors fungieren als fehlertoleranter \textit{"Speicher"}. Acceptors werden in Gruppen gesammelt, die als Quorum bezeichnet werden. Jede an einen Acceptor gesendete Nachricht muss an ein Quorum von Acceptors gesendet werden. Jede Nachricht, die von einem Acceptor abgeschickt wird, hat keine Auswirkung, solange die gleichen Nachrichten nicht vom Quorum der Acceptors abgeschickt wird.
	
	\item[Proposer (Vorschlagender).] Ein Proposer überzeugt die Acceptors, den von einem Client empfangenen Wert zu akzeptieren, sich darauf zu einigen und fungiert als Koordinator, um den Cluster bei Konflikten voranzutreiben.
	
	\item[Learner (Lernender).] Ein Learner fungiert als der \textit{Replikationsfaktor}. Sobald eine Client Anfrage von den Acceptors akzeptiert wurde, kann der Learner seine Rolle erfüllen (die Anfrage ausführen und eine Antwort an den Client senden). Um die Verfügbarkeit des Systems zu verbessern, können zusätzliche Learners hinzugefügt werden.
	
	\item[Leader.] Paxos erfordert \textit{einen speziellen Proposer}, um einen Fortschritt im Algorithmus und im ganzen Cluster zu erzielen. Paxos garantiert einen Fortschritt allerdings nur dann, wenn einer von den Proposers schließlich zum Leader ausgewählt wird. Wenn zwei Prozesse glauben, dass sie führend sind, können sie den Algorithmus blockieren, indem sie ständig widersprüchliche Aktualisierungen vorschlagen. Die Sicherheitseigenschaften bleiben in diesem Fall jedoch erhalten.
\end{description}

Das Konsensverfahren in Paxos hat folgende wichtige Meilensteine \cite{Lamport2001paxos-made-simple}:

\begin{description} 
	\item[Prepare (Vorbereiten).] Ein Proposer erstellt eine Nachricht, die “Prepare” genannt wird und die man mit einer Zahl N identifiziert. Dieser Wert muss unter allen anderen Proposers einzigartig sein. Beispielsweise kann der erste Proposer aus den Zahlen 1, 4, 7, ... auswählen, der zweite Proposer aus 2, 5, 8, ... , der dritte Proposer aus 3, 6, 9, ... . Anschließend schickt der Proposer die Nachrichten an ein Quorum von Acceptors.
	
	\item[Promise (Versprechen).] Jeder Acceptor wartet auf eine Nachricht von jedem Proposer. Wenn ein Acceptor eine Preparenachricht empfängt, muss der Acceptor die Identifikationsnummer N der gerade empfangenen Preparenachricht betrachten. Dabei gibt es zwei Fälle. Wenn N höher ist als jede vorher vorgeschlagene Zahl, muss der Acceptor \textit{Promise}-Nachricht an den Proposer zurückschicken, andernfalls wird der Vorschlag ignoriert. Wenn der Acceptor eine Acceptnachricht zu einem bestimmten Zeitpunkt in der Vergangenheit angenommen hat, muss er die vorher vorgeschlagene Zahl und den entsprechenden akzeptierten Wert an den Proposer schicken.
	
	\item[Accept (Akzeptieren).] Wenn ein Proposer ein Versprechen von einem Quorum aller Knoten erhält, sendet er eine Acceptnachricht mit der Zahl N (wurde in der Prepare Nachricht verwendet) und einem Wert, der vom Proposer ausgewählt wird. Wenn Acceptors bereits einen Wert akzeptiert haben, kann der Proposer nur einen zuvor ausgewählten Wert senden. Das beschreibt die wesentliche Einschränkung des ursprünglichen Singledecree-Paxos, die in Multidecree-Paxos (auch bekannt als multi-Paxos) gelöst wird.

	\item[Accepted (Akzeptieren).] Wenn ein Acceptor eine Accept-Nachricht von einem Proposer erhält, muss er diese Nachricht nur dann akzeptieren, wenn er einem anderen Proposer mit einer höheren Prepare-Zahl noch kein Versprechen gegeben hat. Der Acceptor sollte den akzeptierten Wert registrieren und eine akzeptierte Nachricht an den Proposer und alle Learners senden.

	Wenn Acceptors eine Accept-Anfrage eines Proposers erhalten, bestätigt das den vorgeschlagenen Wert und das Leadership des Proposers. Daher wird Paxos verwendet, um einen Wert und einen Leader in einem Cluster auszuwählen.
\end{description}

In Paxos kann ein einzelner Knoten eine Rolle, mehrere Rollen oder alle Rollen gleichzeitig übernehmen \cite{Lamport2001paxos-made-simple}. Um einen Wert zu akzeptieren, muss jeder Knoten wissen, wie viele Knoten ein Quorum beinhaltet. Darüber hinaus können Paxos-Knoten in der ursprünglichen Version des Algorithmus nicht vergessen, welchen Wert sie früher ausgewählt haben. Es sollte allerdings erwähnt werden, dass es einige Variationen des Paxos-Algorithmus gibt, die das Löschen akzeptierter Werte erlauben.

Ein Durchlauf des Paxos-Algorithms erlaubt nur einen Wert zu wählen. Um einen neuen Wert zu wählen, muss der Algorithmus nochmal gestartet werden. Diesen Algorithmus nennt man \textit{Multi-Paxos} (Abkürzung von Multidecree-Paxos), der sich dem Raft-Algorithmus sehr ähnelt.

Bekannte Produktivsysteme und Werkzeuge, die Paxos als Konsens-Algorithmus verwenden, sind:

\begin{enumerate}
	\item Amazon ECS
	
	\item Apache Cassandra
	
	\item Ceph
	
	\item Neo4j
\end{enumerate}

\subsection{Der Raft-Algorithmus}

Raft ist ein Konsens-Algorithmus zum Verwalten \textit{eines replizierten Logs} \cite{Ongaro14insearch}. Er erzeugt ein Ergebnis, das dem (multi-)Paxos entspricht. Er ist ebenfalls genau so effizient wie Paxos, aber seine Struktur unterscheidet sich von ihm. Es ist wichtig zu beachten, dass Raft kein neuer Algorithmus ist, sondern eine Optimierung und eine inkrementelle Verbesserung des Paxos-Algorithmus \cite{Ongaro14insearch}.

Konsens-Algorithmen entstehen typischerweise im Kontext replizierter Zustandsautomaten. Diese werden normalerweise mithilfe eines replizierten Logs implementiert, wie in Abbildung \ref{fig:rsm} gezeigt wird. Jeder Knoten speichert ein Log mit einer Reihe von Befehlen, die von seinem Zustandsautomaten bearbeitet wird. Jedes Log enthält die gleichen Befehle in der gleichen Reihenfolge, sodass jeder Zustandsautomat die gleiche Folge von Befehlen verarbeitet. Da die Zustandsautomaten deterministisch sind, berechnet jeder den gleichen Zustand und die gleiche Folge von Ausgaben.

Das replizierte Log konsistent zu halten ist die Aufgabe des Konsensus-Algorithmus. Das Konsensmodul an einem Knoten empfängt Befehle von Clients und fügt sie seinem Log hinzu \cite{Ongaro14insearch}. Es kommuniziert mit den Konsensmodulen an anderen Knoten, um sicherzustellen, dass jedes Log schließlich dieselben Werte in derselben Reihenfolge enthält, auch wenn einige Knoten fehlschlagen (siehe Abbildung \ref{fig:rsm}).

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{images/1_replicated_state_machine}
	\caption{Architektur eines replizierten Zustandsautomaten \cite{Ongaro14insearch}.}
	\label{fig:rsm}
\end{figure}

Zu jedem Zeitpunkt ist jeder Knoten in einem von drei möglichen Zuständen sein:

\begin{description} 
	\item[Leader.] Ein Leader verarbeitet alle Client-Anfragen. Wenn ein Client einen Follower kontaktiert, wird er von diesem Follower an den Leader weitergeleitet.
	
	\item[Follower (Anhänger).] Followers sind passiv: sie selber schicken keine Anfragen aus und antworten nur auf mögliche Anfragen des Leader oder eines Kandidaten.
	
	\item[Candidate (Kandidat).] Wenn ein Follower zum Kandidaten wird, darf er sich zum Leader vorschlagen, wobei er vom Quorum aller Knoten unterstützt werden muss.
\end{description}

Im Normalbetrieb es gibt genau einen Leader und alle anderen Knoten sind Followers.

Raft teilt die Zeit in Terms (Amtsperioden) beliebiger Länge. Terms werden mit aufeinanderfolgenden ganzen Zahlen nummeriert. Jeder Term beginnt mit einer Wahl, bei der ein oder mehrere Kandidaten versuchen, Leader zu werden. Wenn ein Kandidat die Wahl gewinnt, fungiert er für den Rest des Terms als Leader. Verschiedene Knoten können die Übergänge zwischen Terms zu verschiedenen Zeiten beobachten. In einigen Situationen übersieht ein Knoten möglicherweise eine Wahl oder sogar ganze Terms. Terms agieren in Raft als eine \textit{"logische Uhr"} und ermöglichen es den Knoten, veraltete Informationen wie z.B. veraltete Leaders zu erkennen. Jeder Knoten speichert eine aktuelle Term-Nummer, die mit der Zeit monoton zunimmt. Der aktuelle Term wird ausgetauscht, wenn Knoten kommunizieren. Ein Term wird gleich nach dem Absturz des aktuellen Leaders beendet \cite{Ongaro14insearch}.

Um die Verständlichkeit zu verbessern, trennt Raft die Schlüsselelemente des Konsenses:

\begin{description} 
	\item[Wahlprozess des Leaders.] Jeder neu gestartete Knoten wird standardmäßig zum Follower. Ein Knoten bleibt im Follower-Zustand, solange er einen \textit{Heartbeat} von einem Leader erhält. Wenn Follower innerhalb einer bestimmten Zeit keine Heartbeat-Nachrichten vom Leader erhält, beginnt der Wahlprozess. Um eine Wahl zu beginnen, erhöht ein Follower den zuletzt gesehenen Term und wechselt in den Kandidatenzustand. Er stimmt dann für sich selbst und schickt RequestVote-Anfragen parallel an alle anderen Knoten im Cluster. Ein Kandidat bleibt in diesem Zustand, bis eine von drei Optionen vorkommt:
	
	\begin{enumerate}
		\item der Kandidat gewinnt die Wahl (ein Kandidat gewinnt die Wahl, wenn er Stimmen von einem Quorum für denselben Term erhält)
		
		\item ein anderer Knoten etabliert sich als Leader
		
		\item ein Zeitraum vergeht ohne Gewinner
		
	\end{enumerate}
	
	\item[Replikation des Logs.] Sobald ein Leader gewählt wurde, beginnt er mit der Bearbeitung der Client-Anfragen. Jede Client-Anfrage enthält einen Befehl, der von den replizierten Zustandsautomaten ausgeführt werden soll. Der Leader hängt den Befehl als einen neuen Eintrag an sein Log an und schickt AppendEntries-Anfragen zu jedem anderen Knoten, um den Eintrag zu replizieren. Wenn der Eintrag sicher repliziert wurde (d.h. auf einem Quorum aller Knoten), wendet der Leader den Eintrag auf seinen Zustandsautomaten an und gibt das Ergebnis dieser Ausführung an den Client zurück. Jeder Logeintrag enthielt einen Befehl inklusive einer Term-Nummer.
\end{description}

Raft wurde als Basis für folgende Systeme genommen:

\begin{enumerate}
	\item HashiCorp Consul
	
	\item RethinkDB
	
	\item Hazelcast
	
	\item CockroachDB
	
	\item etcd
\end{enumerate}

\subsection{Der Vergleich von Paxos und Raft}

Um die Entscheidung zu treffen, mit welchem Algorithmus ein Produktivsystem implementiert werden soll, muss man sie anhand bestimmter Kriterien vergleichen. Die Algorithmen haben natürlich Ähnlichkeiten aber auch Unterschiede.

\begin{description} 
	\item[Ähnlichkeiten.] Paxos und Raft sind schon deswegen sehr ähnlich, weil sie das Problem des Konsenses in verteilten Systemen lösen, was ein Subproblem des Atomic Broadcasts darstellt (Atomic Broadcasts wird in Probleme des Reliable Broadcasts und Konsenses zerlegt). Beide Algorithmen erfüllen Anforderungen an Safety, Availability und Zeitunabhängigkeit.
	
	Sowohl Paxos als auch Raft versuchen, eine Gesamtreihenfolge von Befehlen zu bestimmen, die auf den replizierten Zustandsautomaten angewendet werden sollen. Diese Algorithmen lösen das gleiche Problem und verfolgen auch den gleichen Ansatz: beide wählen einen Leader, der Befehle in seinen Log hinzufügt und zu einem Quorum repliziert. Wenn ein Leader abstürzt, kommt ein Quorum zusammen und wählt einen neuen Leader. Beide Algorithmen garantieren, dass ein Knoten zum Leader ernannt wird, wenn er Stimmen von einem Quorum aller Knoten erhält. Außerdem haben Paxos und Raft immer nur einen Leader. Im Normalbetrieb hängt der Leader jede Operation an sein Log an und fordert andere Knoten auf, dasselbe zu tun. Sobald ein Quorum das getan hat, markiert der Leader den Logeintrag als \textit{"commited"} (erfolgreich repliziert).
	
	Wenn ein Follower glaubt, dass der Leader abgestürzt ist, wird der Follower zum Kandidaten und ersucht alle anderen Knoten für ihn zu stimmen. Wenn der Kandidat Stimmen von der Mehrheit aller Knoten (Quorum) bekommt, wird er zum Leader. Hier enden die Ähnlichkeiten beider Algorithmen.
	
	
	\item[Technische Unterschiede.] Der größte Unterschied zwischen Paxos und Raft ist, wie die Safety Eigenschaft von Paxos und Raft gewährleistet wird.
	
	In Paxos wird ein Follower für jeden Kandidaten stimmen, der um eine Stimme ersucht. Daher muss die Antwort eines Followers alle bei dem Kandidaten fehlenden Logeinträge enthalten. Der Nachteil dieses Verfahrens ist die Belastung des Netzwerks, wenn ein Follower, der lange Zeit nicht erreichbar war, zu einem Kandidaten wird. Dieser Kandidat wird eine lange Liste von fehlenden Einträgen benötigen, was unter anderem eine langsame Zusammenführung aller fehlenden Einträge zur Folge hat. In Raft seinerseits wird ein Follower nur den Kandidaten anerkennen, der mindestens genauso einen aktuellen Log hat, wie der Follower. Generell wollte der Erfinder des Raft-Algorithmus die Einfachheit auf zwei Arten erreichen: die Anzahl von Nachrichten zu reduzieren und den Wahlprozess zu optimieren.
	
	In der Abbildung \ref{fig:logstate} wird ein Cluster mit fünf Knoten abgebildet. Der Knoten 1 ist der Leader, der Knoten 4 (mit zwei Logeinträgen) ist ein Follower, der eine lange Zeit keine Updates von dem Leader erhalten hat. Laut dem Paxos-Algorithmus kann der vierte Follower auch zum Leader gewählt werden. Wenn er tatsächlich zum Leader gewählt wird, muss er alle fehlenden Logeinträge anordnen. Laut Raft also entweder der dritte oder fünfte Knoten. Nur diese beiden Knoten haben einen Log, das zumindestens genauso aktuell ist, wie der Log bei einem Quorum aller Knoten.
	
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{images/2_log_state}
		\caption{Zustand eines Logs \cite{Ongaro14insearch}.}
		\label{fig:logstate}
	\end{figure}
	
	\item[Nicht technische Unterschiede.] Wenn man den Inhalt der ursprünglichen Publikation der Paxos- und Raft-Algorithmen vergleicht, wirkt der Raft-Algorithmus deutlich verständlicher und einfacher zu implementieren. Darauf hat der Autor des Raft-Algorithmus mehrmals hingedeutet \cite{Ongaro14insearch}. Das wurde auch durch eine Reihe von Publikationen bestätigt (Paxos Made Simple \cite{Lamport2001paxos-made-simple}, Paxos Made Moderately Complex \cite{Renesse15paxosmade}, Paxos Made Live \cite{Chandra07paxosmade}), die versucht haben, den ursprünglichen Paxos-Algorithmus simpler zu erläutern. Es ist erwähnenswert, dass die erste von diesen Publikationen vom Autor des Paxos Algorithmus verfasst wurde.
\end{description}

Die Praxisorientiertheit und der Pragmatismus von Raft werden auch durch die Erwähnung bestimmter Technologien für den Nachrichtenaustausch hervorgehoben. Diese Technologie nennt sich \textit{RPC} (Remote Procedure Call). In der Publikation des Paxos-Algorithmus gibt es keine solchen Tipps.

Der Vergleich in der Tabelle \ref{tab:vergleich} zeigt deutlich, dass Raft durch seine Erklärbarkeit und pragmatische Darstellung einen frischen Blick auf moderne verteilte Systeme wirft und die Optimierung der Replikation eine bessere Option für ein Produktivsystem darstellt \cite{Ongaro14insearch}. Das wird auch durch seine in den letzten Jahren schnell gestiegene Popularität bei großen kommerziellen Projekten (z.B. HashiCorp Consul) bestätigt.

\begin{table} \centering
	\begin{tabular}{|p{5cm}|p{5cm}|p{5cm}|} 
		
		\hline
		&  \textbf{Raft} & \textbf{Paxos}\\
		
		\hline
		Veröffentlichung & 2014 & 1998\\
		
		\hline
		Erklärbarkeit des Algorithmus in der ursprünglichen Publikation laut \cite{Ongaro14insearch} & Sehr gut & Mittelmäßig\\
		
		\hline
		Erklärbarkeit des Kerns des Algorithmus laut \cite{Ongaro14insearch}  & Gut & Gut\\
		
		\hline
		Spielraum bei der Implementierung & Weniger Spielraum. Einfach zu implementieren. & Mehr Spielraum. Viele Annahmen müssen getroffen werden.\\
		
		\hline
		Gewährleistung der Safety Eigenschaft & Nur der Kandidat mit dem aktuellsten Stand des Logs wird gewählt. Optimale Nutzung der Netzwerk Ressourcen. & Jeder Kandidat darf gewählt werden. Zusendung fehlender Logeinträge ist erforderlich. Nutzung der Netzwerkressourcen ist nicht optimal.\\
		
		\hline
	\end{tabular}
	\caption{Vergleich der Raft und Paxos Algorithmen}
	\label{tab:vergleich}
\end{table}

\section{Concurrency Modelle für die Umsetzung von verteilten Systemen}

Algorithmen für verteilte Systeme umfassen mehrere Rechner, was ein passendes Concurrency Modell erfordert. Das Concurrency Modell bringt nicht nur die für die Entwicklung des Systems benötigten Bausteine und Abstraktionen, sondern auch ein \textit{Kommunikationsmuster}. Dies wird in Form eines Frameworks und einer Bibliothek zur Verfügung gestellt bzw. direkt in der Programmiersprache implementiert. Dieses Kapitel beschäftigt sich mit diesen Aufgaben.

\subsection{Concurrency in eingebetteten Systemen}

Moderne Hardware- und Softwaresysteme verfügen über mehrere Funktionen. Eine Drohne muss sich z.B. im Raum lokalisieren, ein Modell vom Raum erstellen und die Aktuatoren steuern können. Zusätzlich muss sie mit einer Bodenstation oder mit der Cloud verbunden sein. Die von der Cloud oder von der Bodenstation empfangenen Daten und Signale könnten auch für die Steuerung von Aktuatoren notwendig sein. Das bedeutet, dass diese Funktionen miteinander zu kommunizieren können müssen.

Außerdem muss eine sich in einem Schwarm befindliche Drohne stets in der Lage sein, mit anderen Drohnen zu kommunizieren oder als Schwarm-Leader die von der Cloud oder der Bodenstation empfangenen Daten an den Rest des Schwarms weiterzuleiten. Von Vorteil ist dabei, wenn das Programmiermodell für die Drohnen- und Serversoftware den gleichen Ansatz für die Verteilung von Rechenoperationen und bidirektionale Kommunikation verwendet.

Um diese Anforderungen erfüllen zu können, wird ein Concurrency Modell benötigt. Im Kontext der eingebetteten Systeme muss das Modell folgende Voraussetzungen erfüllen:

\begin{description} 
	\item[Fehlertoleranz.] Ein Fehler in einer Komponente des Systems darf alle anderen Teile nicht zum Absturz bringen. Fehlerdomänen müssen klar definiert und abgekapselt werden. Die fehlerhafte Softwarekomponente muss ersetzt oder erneut gestartet werden.
	
	\item[Bidirektionale Kommunikation.] Concurrent Components des Systems müssen miteinander kommunizieren, idealerweise ohne den intrinsischen Zustand zu teilen. \textit{"Concurrent"} übersetzt auf Deutsch heißt \textit{"nebenläufig"}, und bedeutet, dass zwei Teile eines Programms abwechselnd ausgeführt werden. Das darf nicht mit \textit{"Parallelität"} verwechselt werden (\textit{"parallelism"} auf Englisch), was das Ausführen von zwei Teilen eines Programms zur selben Zeit bedeutet.
	
	\item[Verteiltes Rechnen.] Das Concurrency Modell soll im Idealfall den reibungslosen Aufruf von Remote-Services eines verteilten Systems ermöglichen. Das ist für die Verteilung von Tasks in einem Netzwerk von Drohnen und für eine freie Kommunikation mit der Cloud notwendig, dal ein Schwarm von Drohnen auch ein verteiltes IoT \footnote{Internet of Things oder Intrnet der Dinge} System darstellt. Ein sogenanntes IoT System ist ein Netzwerk von physischen Objekten, die mit Sensoren, Aktuatoren, Kommunikationsmodulen und einer Recheneinheit ausgestattet sind. Eine Drohne verfügt über alle diese Komponenten.
\end{description}

\subsection{Stand der Technik und Vergleich von Concurrency Modellen}

Heutzutage existieren unterschiedliche Concurrency Modelle, die unterschiedliche Hintergründe und Motivationen für ihr Entstehen haben. Erste Modelle wurden 1970 vorgeschlagen und haben sich mit der Zeit geändert, wurden verbessert und an die Anforderungen moderner Software angepasst \cite{Andrews83conceptsand}. Im Folgenden werden die Stärken und Schwächen der Modelle sorgfältig untersucht und verglichen.

\begin{description} 
	\item[Threads und Mutex.] Dies ist ein klassisches Modell, welches in der Industrie schon seit Jahrzehnten im Einsatz ist \cite{Long10javaconcurrency}. Dieses Modell ist auf den zwei Säulen der Concurrency aufgebaut: Thread und Mutex.
	
	\begin{description} 
		\item[Thread.] Ein Thread is ein Ausführungsstrang bzw. eine Ausführungsreihenfolge in der Abarbeitung eines Programms. Ein Thread ist Teil eines Prozesses. In Concurrenten Programmen (\textit{"concurrent programs"} auf Englisch) können zwei Threads gleichzeitig auf eine geteilte Variable oder Ressource zugreifen. Wenn das passiert, ist es möglich, dass die Ressource später in einem inkonsistenten Zustand bleibt. Um das zu vermeiden, müssen folgende Voraussetzungen erfüllt werden: der Zugriff auf alle geteilten Ressourcen muss synchronisiert sein. Sowohl der Schreibvorgang als auch der Lesevorgang muss Synchronisation verwenden. Das wird durch den wechselseitigen Ausschluss bzw. Mutex erreicht.
		
		\item[Mutex (Wechselseitiger Ausschluss).] Der wechselseitige Ausschluss bezeichnet eine Gruppe von Verfahren, mit denen das Problem \textit{des kritischen Abschnitts} gelöst wird. Das Problem tritt dann auf, wenn zwei Threads auf eine Ressource oder eine geteilte Variable zugreifen wollen. Das Mutex-Verfahren verhindert, dass Concurrent Prozesse bzw. Threads gleichzeitig oder zeitlich verschränkt gemeinsam genutzte Datenstrukturen unkoordiniert verändern, wodurch die Datenstrukturen letztendlich in einem inkonsistenten Zustand bleiben können. Die Faustregel für die Verwendung von Mutex lautet: die Kritische Sektion muss so klein wie möglich gehalten werden und so schnell wie möglich verlassen werden.	
	\end{description}
	
	Da dieses Modell so einfach ist, bieten fast alle Programmiersprachen das Modell in unterschiedlicher Form an.
	
	Die primäre Stärke des Threads- und Mutex-Modells ist eine breite Anwendbarkeit \cite{Long10javaconcurrency}. Es kann für eine Vielzahl von Problemen eingesetzt werden. Alle abstrakten Modelle, wie z.B. das \textit{Aktormodell} \cite{Hewitt_artificialintelligence} (actor model) und \textit{CSP} (Communicating Sequential Processes) \cite{Graham_communicatingsequential}, sind auf dessen Basis gebaut. Da Threads und Mutexes sehr nah an die Hardware gebunden sind, können sie sehr effizient sein, wenn sie richtig verwendet werden. Zusätzlich kann man sie sehr einfach in viele Programmiersprachen integrieren.
	
	Außerhalb von einigen experimentellen Systemen mit verteiltem Arbeitsspeicher unterstützen Threads und Mutexes nur Architekturen mit lokalem RAM. Das heißt, Threads und Mutexes sind nicht geeignet für Rechenoperationen, die einen großen Arbeitsspeicher benötigen.
	
	\item[Aktormodell.] Das Aktormodell ist ein universelles Concurrent Programmierungsmodell mit einer besonders breiten Anwendbarkeit. Es ist sowohl für lokale als auch für verteilte Speicherarchitekturen gedacht. Das Modell erleichtert geographische Verteilung und unterstützt Fehlertoleranz und Belastbarkeit \cite{Hewitt_artificialintelligence}.
	Ein Aktor ähnelt einem Objekt in einem objektorientierten Programm. Er sichert die Kapselung des Zustandes und kommuniziert mit den anderen Aktoren mittels Nachrichtenaustausch. Obwohl das Aktormodell ein generischer Ansatz für Concurrency ist, der fast mit jeder Programmiersprache verwendet werden kann, wird es sehr oft mit der Programmiersprache \textit{Erlang} assoziiert.
	
	Im Aktormodell ist ein Aktor die grundlegende Recheneinheit. Ein Aktor hat eine Reihe von Operationen, die er ausführen kann \cite{Hewitt_artificialintelligence}:
	
	\begin{enumerate}
		\item einen anderen Aktor erstellen
		
		\item eine Nachricht schicken (die einzige Möglichkeit zum Kommunizieren im Aktormodell)
		
		\item den inneren Zustand ändern (nur nachdem eine Nachricht empfangen wurde)
	\end{enumerate}

	Die wichtigsten Bausteine eines Aktors sind folgende Bestandteile:
	
	\begin{description} 
		\item[Mailbox.] Eine der wichtigsten Eigenschaften des Aktormodells ist es, dass die Nachrichten asynchron ausgetauscht werden. Anstatt direkt an einen Aktor zugestellt zu werden, werden die Nachrichten in einer Mailbox gespeichert. Das bedeutet, dass die Aktoren von ihren Mailboxes entkoppelt sind und dass sie neue Nachrichten im eigenen Tempo bearbeiten. Dadurch werden sie nicht geblockt während des Nachrichtenaustauschs. Aktoren laufen zwar parallel zueinander, sie verarbeiten Nachrichten aber sequentiell in der \footnote{First In First Out} FIFO-Reihenfolge und gehen nur dann zur nächsten Nachricht über, wenn die Verarbeitung der aktuellen Nachricht abgeschlossen ist.
		
		\item[Adresse.] Ein Aktor darf nur mit den Aktoren kommunizieren, deren Adressen er hat. Ein Akteur kann Adressen anderer Aktoren auf zwei verschiedene Arten erhalten: Adressen von Aktoren, die ein Aktor selber erstellt hat, oder Adressen von Aktoren, die Nachrichten an den Aktor geschickt haben. Ein Aktor kann sich mit mehreren Adressen identifizieren.
	\end{description}
	
	Aktor-Programme neigen dazu, defensive Programmierung zu vermeiden und die Philosophie \textit{"let it crash"} einzuhalten, indem ein Supervisor eines Aktors stetig seine Exceptions (Ausnahmen) bearbeitet. Defensive Programmierung ist ein Ansatz, die sogenannte Fehlertoleranz zu erreichen. Hierbei wird versucht, mögliche Fehler zu antizipieren. Das hat mehrere Vorteile:
	
	\begin{enumerate}
		\item Der Code ist mit einer klaren Trennung zwischen dem \textit{"happy path"} \footnote{Unter "happy path" versteht man im Kontext des Aktormodells die Geschäftslogik eines Programms oder sein Algorithmus} und dem fehlertoleranten Code (Ausnahmebehandlungen) einfacher zu verstehen. Fehlertoleranter Code beschreibt den Vorgang, wenn ein untergeordneter Aktor abstürzt. Das umfasst die Delegierung oder den Neustart eines Aktors.
		
		\item Aktoren sind voneinander getrennt und teilen keinen internen Zustand. Dadurch wird die Gefahr reduziert, dass Fehler in einem Aktor einen anderen Aktor beeinträchtigen. Insbesondere der \textit{Supervisor} eines fehlerhaften Aktors kann nicht abstürzen. Die Funktion des Supervisors kann man mit der Funktion eines Managers vergleichen: er aggregiert mehrere untergeordnete Aktoren, sorgt dafür, dass sie ohne Absturz laufen, löst unberecherechenbare Probleme und - sofern notwendig -  delegiert das Problem an einen übergeordneten Supervisor.
	\end{enumerate}

	Einer der Hauptvorteile im Vergleich zu anderen Concurrency Modellen ist, dass das Aktormodell die Verteilung unterstützt \cite{Hewitt_artificialintelligence}. Das Senden einer Nachricht an einen Aktor auf einem anderen Rechner ist genauso einfach, wie eine Nachricht an einen lokalen Aktor zu schicken. Dabei ist zu beachten, dass die lokalen Aktoren voneinander isoliert sind und sie keinen Arbeitsspeicher miteinander teilen.
	
	Das Aktormodell ist eines der am verbreitesten Programmierungsmodelle. Es bietet nicht nur Unterstützung für Concurrency, sondern auch Verteilung, Fehlererkennung und Fehlertoleranz. Als solches ist es sehr gut für die Art von Programmierproblemen geeignet, mit denen wir in der heutigen zunehmend verteilten Welt konfrontiert sind.
	
	\item[Funktionale Programmierung.] Die Concurrent Programmierung in imperativen Sprachen - wie beispielsweise Java - ist aufgrund eines gemeinsamen veränderlichen Zustands schwierig. Funktionale Programmierung macht Concurrency einfacher und sicherer, indem gemeinsam genutzte veränderbare Zustände eliminiert werden. Auf unveränderliche Daten kann zugegriffen werden, ohne mehrere Threads zu stoppen. Das macht die Funktionale Programmierung so geeignet, wenn es um Concurrency und Parallelität geht. Funktionale Programme haben \textit{keinen veränderlichen Zustand}, was durch folgende Konzepte erreicht wird:
	
	\begin{description} 
		\item[Pure functions (Reine Funktion).] Reine Funktion ist eine Funktion mit folgenden Eigenschaften: ihr Rückgabewert ist für dieselben Argumente immer gleich und ihre Auswertung hat keine Nebenwirkungen (Änderung einer globalen Variable). Somit ist eine reine Funktion ein rechnerisches Analogon einer mathematischen Funktion \cite{Armstrong93concurrentprogramming}.
		
		\item[Referential transparency (Referentielle Transparenz).] Ein Ausdruck wird als referentiell transparent bezeichnet, wenn er durch seinen entsprechenden Wert ersetzt werden kann, ohne das Verhalten des Programms zu ändern. Das bedeutet, dass eine Funktion keine Variablen aus einer globalen Umgebung verwenden darf. Ein Ausdruck, der nicht referentiell transparent ist, wird als referentiell \textit{"opaque"} (nicht transparent) bezeichnet \cite{Armstrong93concurrentprogramming}. Der Hauptnachteil von Sprachen, die referenzielle Transparenz erzwingen, besteht darin, dass sie die Operationen, die auf natürliche Weise zu einem imperativen Programmierstil gehören, umständlicher und weniger verständlich machen.
	\end{description}
	
	Reine Funktionen und Referentielle Transparenz ermöglichen Parallelisierung von Berechnungen oder den Betrieb in einer Concurrent Umgebung mit sehr wenig Aufwand. Da der Code im funktionalen Stil den veränderbaren Zustand eliminiert, sind die meisten Concurrency Fehler, die in traditionellen Thread und Mutex-basierten Programmen auftauchen, unmöglich. Es wird erwartet, dass der funktionale Code weniger effizient ist als das imperative Äquivalent. Obwohl es Auswirkungen auf die Leistung für einige Arten von Aufgaben hat, ist die Leistungsminderung üblicherweise relativ klein. Eine solch geringe Leistungsminderung wird meist zugunsten der verbesserten Skalierbarkeit und Robustheit in Kauf genommen.
	
	\item[Communicating Sequential Processes (CSP).] CSP ist eine \textit{Prozessalgebra} zur Beschreibung von Interaktionen zwischen kommunizierenden Prozessen. Die Idee wurde 1978 als imperative Sprache von Tony Hoare erstmals vorgestellt, dann von ihm zu einer formalen Algebra ausgebaut und 1985 mit der Veröffentlichung des Buchs mit dem gleichnamigen Titel \textit{"Communicating Sequential Processes"} berühmt \cite{Graham_communicatingsequential}. Die Programmiersprachen Go \footnote{Go ist eine kompilierbare Programmiersprache, die Nebenläufigkeit unterstützt und über eine automatische Speicherbereinigung verfügt \cite{Westrup14usingthe}. Entwickelt wurde Go von Google Mitarbeitern} beinhaltet praktische Implementierungen der CSP und stellt - im Gegensatz zu vielen neuen prozeduralen und objektorientierten Programmiersprachen - kein Threading Modell für Concurrency zur Verfügung. Concurrency in Go ist ressourcenschonend und ist wesentlich einfacher zu verwalten als übliche Thread Pools. Als Basis dafür fungieren zwei Konzepte:
	
	\begin{description} 
		\item[Goroutines.] Goroutine ist eine Funktion, die auf einem OS-Thread unabhängig von anderen Goroutinen läuft \cite{Westrup14usingthe}. Das bedeutet, dass ein OS-Thread mehrere Goroutinen abwechselnd ausführt. Goroutinen haben sehr kleinen Overhead für den Stack von nur ein paar Kilobytes.
		
		Vorteile von Goroutinen werden klar, wenn man einen Thread Pool von Threads und einen Thread Pool von Goroutinen vergleicht. Thread Pool ist ein guter Weg, um CPU-gebundene Tasks auszuführen. Das sind die Tasks, die einen Thread für eine kurze Zeit aus dem Pool nehmen und am Schluss dem Thread zurückgeben. Wenn so ein Thread aus dem Pool für Netzwerkkommunikation ausgeliehen wird und der Netzwerkcall unbestimmt lang dauert, verschwindet der Nutzen des Thread Pools komplett. Wäre es ein Pool von Goroutinen, könnte man mit weniger Aufwand ressourcenschonender programmieren.
		
		\item[Channels.] Ein Channel ist eine Pipeline, die zwei miteinander kommunizierende Goroutinen verbindet. Über einen Channel können Goroutinen strukturierte Daten austauschen. Ein Channel ist eine thread-safe Queue. Jede Goroutine mit einer Referenz auf die Queue kann Nachrichten an einer Seite einfügen und an der anderen Seite entfernen. Im Gegensatz zum Aktormodell, in dem Nachrichten von und an bestimmte Aktoren geschickt werden, muss hier der Sender den Empfänger nicht kennen und umgekehrt. Channel ist ein primärer Datentyp, der Go mit dem Stammbaum von CSP bezogenen Programmiersprachen verbindet.
	\end{description}

	Der Slogan von Go im Bezug auf Concurrency heißt \textit{“Do not communicate by sharing memory; instead, share memory by communicating”} \cite{Westrup14usingthe}.
	
\end{description}

\begin{table} \centering
	\begin{tabular}{|P{2.5cm}|P{2.5cm}|P{2.5cm}|P{2.5cm}|P{2.5cm}|} 
		\hline
		&  \textbf{Aktormodell} & \textbf{CSP} & \textbf{Funktionale Programmierung} & \textbf{Threads und Mutex}\\
		
		\hline
		Primär Fokus & Fehlertoleranz und verteiltes Rechnen & Flexibilität und Ausdruckskraft & Nebeneffektefreiheit durch Immutability & Effizienz und breite Anwendbarkeit\\
		
		\hline
		Entkopplung von Producer-Consumer & - & + & nicht anwendbar & nicht anwendbar\\

		\hline
		Direktes Teilen des Zustands & - & - & - & +\\
		
		\hline
		Kommunikationsmuster & standardmäßig bidirektional zwischen den Aktoren & standardmäßig bidirectional zwischen den Coroutinen, unidirektional mit einer “receive-only” oder “send-only” Queue & nicht anwendbar & exklusive Nutzung von Arbeitsspeicher-bereichen\\
		
		\hline
		Struktur von ausführbaren Codeeinheiten & Baum & flach & flach & flach\\
		
		\hline
		Hinzufügen neuer Nachrichten durch & Neue Match Cases innerhalb des Aktors & Weitergabe von neuen Queues in eine Goroutine & nicht anwendbar & nicht anwendbar\\

		\hline
		Fehlertoleranz by Design & + & - & - & -\\
		
		\hline
		Verteiltes Rechnen standardmäßig & + & - & - & -\\
		
		\hline
	\end{tabular}
	\caption{Vergleich von Concurrency Modellen.}
	\label{tab:vergleichConcurrencyModelle}
\end{table}

Laut Tabelle \ref{tab:vergleichConcurrencyModelle} erfüllt nur das Aktormodell die Anforderungen an das Concurrency Modell für ein Drohnensystem. Der Grund dafür ist, dass das Aktormodell standardmäßig Fehlertoleranz und verteiltes Rechnen unterstützt, ein direktes Teilen des Zustands verbietet und den Aufbau einer baumartigen Struktur von ausführbaren Einheiten ermöglicht. Jedoch müssen die Eigenschaften des Aktormodells noch in Detail betrachtet werden.

Das Actor Modell war kein Durchbruch in der Informatik, sondern eine inkrementelle Verbesserung von Ideen des \textit{Lambda-Kalküls}, des Nachrichtenaustauschs und der Interprozesskommunikation. Das Lambda-Kalkül diente als eine Inspiration für den Interpreter der Programmiersprache Lisp, die ihrerseits den Austausch von geteilten Datenstrukturen ohne Parallelität ermöglicht hat. Danach war Simula 67 Pionier bei der Verwendung des Nachrichtenaustauschs für Berechnungen aller Art, motiviert durch diskrete Simulationsanwendungen. 1971 beschäftigte sich der Erfinder des Actor Modells mit der Programmiersprache Smalltalk-71 und war erstaunt von der Komplexität des Nachrichtenaustauschs. Das führte 1973 zur Veröffentlichung vom ersten Whitepaper zum Thema \textit{"Actor Model"}. Das Actor Modell, das wir heute kennen, ist ein Ergebnis von einer Reihe von Dissertationen und wurde erst 1985 endgültig formalisiert.

Die grundlegende Herausforderung bei der Definition des Aktormodells ist, dass es keinen globalen Zustand bereitstellt. Das bedeutet, dass ein Rechenschritt innerhalb eines Aktors den Zustand von dem ganzen System  nicht ändert. Der Zustand wird in dem Fall zwischen mehreren in das System involvierten Aktoren geteilt. Da jeder Aktor seine eigenen lokalen Daten verwaltet, muss man sich um gleichzeitige Zugriffe auf diese Daten keine Sorgen machen. Deshalb wird das Aktormodell auch als \textit{"shared-nothing"} Modell bezeichnet \cite{Hewitt_artificialintelligence}.

Die ursprüngliche Definition des Aktmodells befasste sich mit einem abstrakten Aktor ohne einer Anknüpfung zu einer bestimmten Technologie. In einem Aufsatz aus dem Jahr 2010 hat der Erfinder des Aktormodells Carl Hewitt das von ihm entwickelte Modell im Kontext der modernen verteilten Systemen (Rechenzentren und Webdienste) überdacht und noch folgende Herausforderungen gefunden \cite{hewitt2015actor}:

\begin{description} 
	\item[Skalierbarkeit.] Herausforderung, um Concurrency sowohl lokal als auch nicht lokal zu skalieren.
	
	\item[Transparenz.] Überbrückung der Kluft zwischen lokaler und nicht lokaler Concurrency. Transparenz ist derzeit ein umstrittenes Thema. Einige Wissenschaftler befürworten eine strikte Trennung zwischen lokaler Concurrency mit Concurrent Programmiersprachen (z.B. Java und C\#) und nicht lokaler Concurrency mit REST (Representational state transfer) für Webdienste. Eine klare Trennung schafft Probleme, wenn es wünschenswert oder notwendig ist, von lokalen Diensten auf Webdienste umzusteigen.
	
	\item[Inkonsistenz.] Inkonsistenz ist ein häufiges Problem bei großen Informationssysteme. Zahlreiche Algorithmen aus dem Bereich verteilter Systemen sind dazu ausgerichtet, diese Inkonsistenzen zu eliminieren oder ihnen auszuweichen.
\end{description}

\subsection{Umsetzung des Aktormodells in höheren Programmiersprachen}

Im Allgemeinen gibt es für die Kapselung von Abstraktionen in der Informatik drei Ansätze: ein Framework, eine Bibliothek oder die direkte Umsetzung in der Programmiersprache. Das Aktormodell wird meistens in Form einer Bibliothek oder - in einigen Programmiersprachen - direkt implementiert.

Eine der Bekanntesten Implementierungen des Actor Models ist die Programmiersprache Erlang \cite{Armstrong93concurrentprogramming}. Diese bringt 3 Konzepte zusammen: funktionale Programmierung, verteilte Programmierung und das Aktormodell. Joe Armstrong, der geistige Vater von Erlang, bezeichnet die Sprache am liebsten als \textit{"nebenläufig ausgerichtete Programmiersprache"}, in der Prozesse die wichtigsten Objekte sind. Erlang ist eine der wenigen funktionalen Programmiersprachen, die in der Industrie eingesetzt wird. Insbesondere Telefon- und Netzwerkausrüster setzen Erlang aufgrund seiner guten Skalierbarkeit und Parallelität ein. Die Entwicklung von Erlang begann im Jahre 1986 bei Ericsson. 1998 enthielt der neue \textit{Ericsson AXD 301 Switch} schon mehr als eine Million Zeilen von Erlang Code. Damals hat Erlang mit seinen Paradigmen \textit{"alles ist ein Prozess"}, \textit{"die einzige Kommunikation ist der Nachrichtenaustausch"}, \textit{"keine lokale Fehlerbehandlung"} und \textit{"reibungslose Updates ohne Ausfallzeit”"} für Ericsson die Verfügbarkeit von 99.999999999\% (sogenannte “neun Neuens”) gewährleistet. 2014 berichtete Ericsson, dass Erlang in der Entwicklung von Software für GPRS, UMTS und LTE Netzwerke verwendet wurde. Die Telekommunikationsindustrie ist nicht das einzige Beispiel, wo Erlang erfolgreich eingesetzt wurde. Seit der Veröffentlichung als Open Source wird die Programmiersprache sehr erfolgreich bei Vocalink (ein Unternehmen von MasterCard), Klarna (Sofort Überweisungen) und WhatsApp eingesetzt.

Ein anderer Ansatz ist die Implementierung des Aktormodells in Form einer Bibliothek. Nach der auf Wikipedia veröffentlichten Liste von Actor Bibliotheken existieren mindestens 51 solcher Projekte, die in der aktiven Entwicklungsphase sind. Die fünf populärsten Bibliotheken sind in der Tabelle \ref{tab:vergleichBibliotheken} aufgelistet.

\begin{table} \centering
	\begin{tabular}{|p{1.5cm}|p{4cm}|p{3.5cm}|p{3.5cm}|} 
		\hline
		\textbf{Name} & \textbf{Programmiersprache} & \textbf{Sterne auf Github} & \textbf{Contributors auf Github}\\
		
		\hline
		Ray & Python & 13 900 & 1 200 \\
		
		\hline
		Vert.x & Java & 11 500 & 200 \\
		
		\hline
		Akka & Scala & 11 200 & 700 \\
		
		\hline
		Orleans & C\# & 7 100 & 300 \\
		
		\hline
		Actix & Rust & 5 800 & 90 \\

		\hline
	\end{tabular}
	\caption{Popularität von Bibliotheken, die das Aktormodell implementieren (Stand 23.12.2020).}
	\label{tab:vergleichBibliotheken}
\end{table}

Obwohl das Aktormodell schon 1970 definiert wurde, gewann es erst nach dem Entstehen von Erlang und Scala an Popularität (bzw. mit der Umsetzung des Aktormodells für diese Programmierungssprachen) \cite{Odersky04anoverview}.

Was Scala im Bezug auf das Aktormodell auszeichnet, ist die Tatsache, dass es zwei von der Scala Community akzeptierte Implementierungen des Aktormodells gibt: die erste Implementierung ist wie ein Teil der Programmierungssprache umgesetzt, die zweite ist in Form einer Bibliothek realisiert und heißt \textit{Akka}. Akka unterstützt unterschiedliche Modelle von Concurrency mit einem starken Fokus auf die Aktor-basierte Concurrency. Es ist nicht überraschend, dass die primäre Inspiration für Akka Erlang war.


\textbf{Scala Aktoren.} Laut der Grundidee von Aktoren in Scala, sollen sie nicht nur eine verständliche sondern auch eine leichtgewichtige Alternative zur üblichen parallelen Programmierung in der JVM mit Java Threads darstellen \cite{Odersky04anoverview}. Diese Leichtgewichtigkeit lässt sich jedoch nicht erreichen, wenn jeder Aktor durch einen Thread abgebildet wird, da Threads in der JVM einen sehr hohen Speicherverbrauch haben, um Threads anzuhalten, ihren Zustand zu speichern und fortzusetzen. Dieses sogenannte Context-Switching ist sehr aufwändig. Scala nutzt hierfür bspw. ein eigenes Threading-Modell und nicht das des darunterliegenden Betriebssystems. Eine Alternative zum thread-basierten ist ein ereignis-gesteuerter Ansatz, wofür man üblicherweise bestimmte Event-Handler in der Ausführungsumgebung registriert und diese beim Eintreten dieser Ereignisse aufgerufen werden. Dies nennt man \textit{"Inversion of Control"}, da man den Kontrollfluss des Programms an die Umgebung abgibt. Diese entscheidet, wann diese Funktionen aufgerufen werden.

In Scala gibt es event-basierte Aktoren ohne inversion of control und Thread-basierte Aktoren \cite{scalaactors}. Beide Arten von Aktoren sind gemeinsam im Actor Trait definiert. Innerhalb der Empfangsmethode wird ein Pattern-Matching durchgeführt, um zu überprüfen, ob der Aktor diese Nachricht annehmen kann. Ist dies der Fall, wird die entsprechende Berechnung durchgeführt. Ist der Aktor mit der Verarbeitung einer Nachricht beschäftigt, kommt die gesendete Nachricht in eine Queue und wird später verarbeitet. Kann der Aktor die Nachricht nicht bearbeiten, wird sie verworfen.

Thread-basierte Aktoren eignen sich bspw. für I/O Operationen, welche ohnehin blockiert wären, wohingegen die event-basierten Aktoren in sehr großer Anzahl verwendet werden können. Um event-basierte Aktoren in so großer Anzahl zu ermöglichen, wird ein angehaltener Aktor, welcher bspw. in seiner receive Methode auf den Empfang einer Nachricht wartet, in Scala nicht durch einen Thread abgebildet. Weitere wichtige Eigenschaften von Akka und dem Aktormodell werden im nächsten Kapiteln behandelt.

\subsection{Umsetzung des Aktormodells in eingebetteten Systemen}

Unter den höheren Programmiersprachen existieren zahlreiche Frameworks für das Aktormodell und nebenläufige Berechnungen. Im Embedded Bereich, zu dem unbemannte Luftfahrzeuge gehören, schaut die Situation anders aus.

\begin{description} 
	\item[Abstraktionsschicht der Hardwaresprachen.] Die Sprachen, die die Hardware von eingebetteten Systemen beschreiben und mit denen sie sich programmieren lassen, gehen mit sehr spezifischen Komponenten um, sind an Hardware sehr eng gebunden und stellen keine Abstraktionen zur Verfügung. Darüber hinaus werden diese Sprachen als sogenannte \textit{Hardwarebeschreibungssprachen} bezeichnet. Das heißt, dass sie keine Programmiersprachen sind, obwohl sie Objekte beschreiben, deren Aufgabe meist die Informationsverarbeitung ist. Der Entwicklungsprozess für Embedded Geräte umfasst vor allem die Erstellung eines Modells für die Hardware und die Erzeugung einer textbasierten Beschreibung für das System. Ein leuchtendes Beispiel aus dieser Sprachfamilie ist die Hardwarebeschreibungssprache VHDL.
	
	Hardwarebeschreibungssprachen werden an dieser Stelle erwähnt, weil die Probleme, die man mit Modellen für Concurrency in höheren Programmiersprache löst, in der Embedded Welt und bei der Beschreibung von Hardware mit sehr ähnlichen Abstraktionen in modellbasierten Design- und Simulationswerkzeugen gelöst werden. Das heißt, Modelle aus den beiden Welten (Softwareentwicklung und Systementwicklung) haben dieselben Wurzeln, aber die Interpretationen unterscheiden sich bei ausführlicher Betrachtung. Eines der Projekte, das zum Ziel hat, Modellierung, Simulation und das Design von nebenläufigen, eingebetteten und Echtzeitsystemen einfacher zu machen, ist \textit{Ptolemy II} \cite{Brooks05heterogeneousconcurrent}.
	
	
	\item[Modellierung.] Ptolemy II ist ein Open Source Framework, das Experimente mit aktororientiertem Design unterstützt \cite{Brooks05heterogeneousconcurrent}. Im Kontext von Ptolemy II sind Aktoren Softwarekomponenten, die gleichzeitig ausgeführt werden und über Nachrichten kommunizieren, die über miteinander verbundene Ports gesendet werden. Ein Modell ist eine hierarchische Verbindung von Aktoren. Ein Schwerpunkt des Projekts liegt darin, heterogene Kombinationen von Berechnungsmodellen, kontinuierlichen Zeitmodellen mit Zustandsmaschinen und synchronen/reaktiven Systemen mit Zustandsmaschinen zu verstehen \cite{Xiong02anextensible}.
	
	Ptolemy II konzentriert sich auf die aktororientierte Modellierung komplexer Systeme und bietet einen Ansatz für nebenläufige Berechnungen. Der zentrale Begriff bei der hierarchischen Modellzerlegung ist die Domain, die ein bestimmtes Berechnungsmodell implementiert. Technisch dient eine Domain dazu, den Steuerungs- und Datenfluss zwischen Komponenten von der tatsächlichen Funktionalität einzelner Komponenten zu trennen.
	
	Ptolemäus II ermöglicht die Verwendung einiger Domains, die im  Systemdesign, der Modellierung und der Simulation verwendet werden:
	
	\begin{description} 
		\item[Dataflow.] Die Ausführung eines Aktors in der Dataflow-Domain besteht aus einer Folge von \textit{"Firings"}, wobei jeder Firing als Reaktion auf die Verfügbarkeit von Eingabedaten erfolgt \cite{Brooks05heterogeneousconcurrent}. Ein \textit{Firings} ist eine für gewöhnlich kleine Berechnung, die die Eingabedaten verwendet und Ausgabedaten erzeugt. Dataflow Modelle sind ideal für die Darstellung von Streaming Systemen, bei denen Sequenzen von Datenwerten in relativ regelmäßigen Mustern zwischen Komponenten fließen. Signalverarbeitungssysteme, wie Audio- und Videosysteme, passen besonders gut zusammen.
		
		\item[Prozessnetzwerke.] In der Domain des Prozessnetzwerks stellen Aktoren Prozesse dar, die über FIFO-Queues (konzeptionell unendlicher Kapazität) kommunizieren \cite{Brooks05heterogeneousconcurrent}. Das Schreiben in die Queues ist eine nicht-blockierende Operation, während das Lesen aus einer leeren Queue einen Reader Prozess blockiert. Die einfache Strategie des blockierenden Lesens und des nicht-blockierenden Schreibens gewährleistet den Determinismus des Modells. Ein Prozessnetzwerk eignet sich zur Beschreibung nebenläufiger Prozesse, die durch einen asynchronen Nachrichtenaustausch miteinander kommunizieren. Prozessnetzwerke kann man als das Aktormodell betrachten, das in höheren Programmiersprachen verwendet wird. Mit Prozessnetzwerken kann man auch die Vorgänge in verteilten Systemen darstellen. Der einzige Unterschied zur ursprünglichen Definition eines verteilten Systems besteht darin, dass die Nachrichten in der Versandreihenfolge zwar zugestellt werden, die genaue Zeit allerdings nicht vorher festgelegt ist. In der klassischen Modellierung von verteilten Systemen wird angenommen, dass alle Nachrichten eventuell zugestellt werden müssen, jedoch die Reihenfolge, in der sie ankommen, ist nicht definiert.
		
		\item[Rendezvous.] Die Rendezvous-Domain ähnelt einem Prozessnetzwerk insofern, dass Aktoren gleichzeitige Prozesse darstellen. Im Gegensatz zur Semantik \textit{"Fire And Forget"} von Prozessnetzwerken kommunizieren Aktoren in der Rendezvous-Domain jedoch durch den atomaren sofortigen Datenaustausch. Wenn ein Aktor Daten an einen anderen Aktor sendet, wird der Sender blockiert, bis der Empfänger empfangsbereit ist. Wenn ein Aktor versucht, Eingabedaten zu lesen, wird er auch blockiert, bis der Sender der Daten bereit ist, die Daten zu senden. Infolgedessen bleibt der Prozess, der als erster einen Treffpunkt erreicht, blockiert, bis der andere Prozess denselben Treffpunkt erreicht. In dieser Domain ist es auch möglich, Mehrweg-Rendezvous zu erstellen, bei dem mehrere Prozesse den Rendezvous Punkt erreichen müssen, bevor ein Prozess fortgesetzt werden kann. Die Rendezvous-Domäne ist besonders nützlich für die Modellierung von Problemen, bei denen eine einzelne Ressource von mehreren asynchronen Prozessen gemeinsam genutzt wird \cite{Brooks05heterogeneousconcurrent}.
		
		Die Tatsache, dass die Rendezvous-Domain auf der Prozessalgebra von Tony Hoare basiert, macht klar, dass das Communicating Sequential Processes Concurrency Modell und Rendezvous dieselben Wurzeln in der akademischen Forschung haben.
		
		\item[Discrete Event.] In der Discrete Event Domain kommunizieren Aktoren über Events, die auf einer Zeitleiste platziert sind \cite{Brooks05heterogeneousconcurrent}. Jedes Event hat einen Wert und einen Zeitstempel. Aktoren verarbeiten Events in chronologischer Reihenfolge. Die von einem Aktor erzeugten Ausgabeevents müssen nicht früher als die verbrauchten Eingabeevents vorliegen. Anders gesagt sind Aktoren in der Discrete Event Domain kausal.
		
		Die Ausführung dieses Modells verwendet eine globale Event-Queue. Wenn ein Akteur ein Event generiert, wird das Event entsprechend seines Zeitstempels in die Queue eingefügt. Während jeder Iteration des Modells werden die Events mit dem kleinsten Zeitstempel aus der globalen Event Queue entfernt und der entsprechende Aktor wird ausgelöst.
		
		Discrete Event eignet sich gut für die Modellierung des Verhaltens komplexer Systeme im Zeitverlauf. Es kann beispielsweise Netzwerke, digitale Hardware und Finanzsysteme modellieren.
	\end{description}
\end{description}

Die oben aufgelisteten Modelle sind Beispiele davon, wie Hardwarebeschreibungssprachen und höhere Programmiersprachen verflochten sind. Die Anwendung von ähnlichen Abstraktionen bei der Systementwicklung ist sinnvoll und etabliert eine \textit{"gemeinsame Sprache"} für Entwickler von Hardware- und Softwarekomponenten.

Am Beispiel des Process Network Modells für eingebettete Systeme sieht man, dass es dieselbe Wurzeln wie die Aktormodelle für höhere Programmiersprachen hat.

Viele von den heutzutage entwickelten Systemen kombinieren heterogene und oft komplexe Subsysteme. Ein modernes Auto kann einen komplexen Motor, elektronische Steuergeräte, Traktionskontrollsysteme, Karosserieelektronik (zur Steuerung von Fenstern und Türschlössern), Infotainmentsysteme, Klimatisierung und Belüftung sowie eine Vielzahl von Sicherheits-Subsystemen (wie Airbags) kombinieren. Jedes Subsystem wird mit einer Kombination aus Software, Elektronik und mechanischen Teilen realisiert. Die Entwicklung derart komplexer Systeme ist sehr anspruchsvoll, da zum Teil selbst kleinste Teilsysteme mehrere Ingenieurdisziplinen umfassen.

\section{Fazit}

Der Drohnenschwarm ist ein komplexes System, das aus mehreren Rechenknoten besteht, die Kommunikation, Synchronisation und Koordination für die Zusammenarbeit brauchen. Die Zusammenarbeit im Offline-Modus (ohne Verbindung zur Cloud) oder mit der begrenzten Erreichbarkeit des zentralen Servers ist nur dann möglich, wenn es im Schwarm einen Leader gibt, der die Konsistenz von Daten garantiert und sich mit der Cloud synchronisiert. Für die Lösung dieser Aufgabe gibt es mehrere Algorithmen, von denen sich besonders Raft mit seiner Erklärbarkeit und Einfachheit auszeichnet.

Für die Umsetzung eines verteilten Systems ist auch ein Concurrency Modell nötig. Verteilte Systeme stellen folgende Anforderungen an das Modell: standardmäßige Fehlertoleranz und Unterstützung des verteilten Rechnens. Von den Modellen, die in diesem Kapitel behandelt werden, ist das Aktormodell dafür am besten geeignet.

Eine gute Perspektive auf die Theorie und Modellierung von nebenläufigen, eingebetteten Systemen gibt Ptolemy II. Ptolemy II hat als ein Design-Werkzeug und als Framework ein sehr interessantes Problem aufgeworfen, was für die Entwicklung von aktorbasierten Systemen wichtig ist. Wie müssen die Aktoren aufgegliedert sein? Wie viele Aktoren muss es in einem System geben? Die Antwort lautet: wenn die Akteure relativ klein sind - also für jede Kommunikation weniger Berechnungen durchführen - kann das Overhead von Multithreading und Kommunikation zwischen Threads die Leistungsvorteile der parallelen Ausführung überwältigen. Daher sollten Ingenieure Leistungsvorteile nur für große oder mittelgroße Aktore erwarten.